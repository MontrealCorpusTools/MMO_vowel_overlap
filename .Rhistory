p_F1_interaction_empirical <- raleigh_data %>%
group_by(speaker, vowel, context) %>%
summarize(F1=mean(F1)) %>%
ungroup() %>%
ggplot(aes(x=vowel, y=F1, color=context)) +
geom_point() +
scale_y_reverse() +
labs(title='empirical') +
geom_line(aes(group=interaction(context, speaker))) +
ylim(0.2, -1.2) +
theme_minimal() +
theme(axis.title.y=element_blank())
p_F2_interaction_modelled <- sp_estimates %>%
ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
geom_point(shape=24, show.legend=FALSE) +
scale_y_reverse() +
# labs(title='modelled') +
geom_line(lty=2, aes(group=interaction(context, speaker)),
show.legend=FALSE) +
ylim(1.5, 0.3) +
theme_minimal()
p_F2_interaction_empirical <- raleigh_data %>%
group_by(speaker, vowel, context) %>%
summarize(F2=mean(F2)) %>%
ungroup() %>%
ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
geom_point(shape=24) +
scale_y_reverse() +
# labs(title='empirical') +
geom_line(lty=2, aes(group=interaction(context, speaker))) +
ylim(1.5, 0.3) +
theme_minimal() +
theme(axis.title.y=element_blank())
(p_F1_interaction_modelled + p_F1_interaction_empirical) /
(p_F2_interaction_modelled + p_F2_interaction_empirical)
sp_estimates <- fitted_sp %>%
group_by(speaker, vowel, context) %>%
mean_qi() %>%
ungroup()
av_preds <- expand_grid(context = factor(levels(model$data$context),
levels=levels(model$data$context)),
vowel = factor(levels(model$data$vowel),
levels=levels(model$data$vowel)))
sp_preds <- expand_grid(context = factor(levels(model$data$context),
levels=levels(model$data$context)),
vowel = factor(levels(model$data$vowel),
levels=levels(model$data$vowel)),
speaker = unique(model$data$speaker))
num_draws=100
num_points=25
fitted_sp <- sp_preds %>%
add_epred_draws(model,
re_formula = ~(1+context*vowel|
q|speaker),
ndraws=num_draws) %>%
ungroup() %>%
pivot_wider(names_from=".category", values_from=".epred")
fitted_av <- av_preds %>%
add_epred_draws(model,
re_formula=NA, # needs to be NA (which keeps it empty), rather than NULL, which is the "default," i.e., complete random effects structure
ndraws=num_draws) %>%
ungroup() %>%
pivot_wider(names_from=".category", values_from=".epred")
predicted_sp <- sp_preds %>%
add_predicted_draws(model,
re_formula = ~(1+context*vowel|
q|speaker),
ndraws=num_draws) %>%
ungroup() %>%
pivot_wider(names_from=".category", values_from=".prediction")
predicted_av <- av_preds %>%
add_predicted_draws(model,
re_formula=NA,
ndraws=num_draws) %>%
ungroup() %>%
pivot_wider(names_from=".category", values_from=".prediction")
sp_estimates <- fitted_sp %>%
group_by(speaker, vowel, context) %>%
mean_qi() %>%
ungroup()
? mean_qi
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(adehabitatHR)
library(arm)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(adehabitatHR)
library(arm)
library(tidybayes)
library(patchwork)
library(ggrepel)
corpus_names <- c("SLAAP-NorthTown-Anglo",
"SLAAP-NorthTown-Latinx",
"Sunset",
"Hastings",
"SOTC")
metadata_filename <- "spade_dialects_2023_06_14.csv"
speaker_metadata_full <- read.csv(paste("../data/", metadata_filename, sep = ""))
speaker_metadata <- speaker_metadata_full %>%
filter(corpus %in% corpus_names)
subcorpora_counts <- speaker_metadata %>%
group_by(corpus) %>%
count(dialect)
View(subcorpora_counts)
View(speaker_metadata)
paste("../data/spade-", corpus_name, "_formants*.csv",
sep = "")
corpus_name='SOTC'
paste("../data/spade-", corpus_name, "_formants*.csv",
sep = "")
data <- read.csv(paste("../data/spade-", corpus_name, "_formants*.csv",
sep = ""))
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")
)
setwd("~/GitHub/MMO_vowel_overlap/Interspeech_analysis/scripts")
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")) %>%
group_by(speaker)
View(data)
View(speaker_metadata)
corpus_name <- 'Hastings'
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = ""))
View(data)
unisyn_aae <- read.csv("../data/aae_keysymbols.csv")
unisyn_can <- read.csv("../data/can_keysymbols.csv")
unisyn_edi <- read.csv("../data/edi_keysymbols.csv")
unisyn_gam <- read.csv("../data/gam_keysymbols.csv")
unisyn_lds <- read.csv("../data/lds_keysymbols.csv")
unisyn_rpx <- read.csv("../data/rpx_keysymbols.csv")
unisyn_sca <- read.csv("../data/sca_keysymbols.csv")
unisyn_xxx <- read_lines("../data/unilex")
unisyn_cmb <- bind_rows(`aae` = unisyn_aae,
`can` = unisyn_can,
`edi` = unisyn_edi,
`gam` = unisyn_gam,
`lds` = unisyn_lds,
`rpx` = unisyn_rpx,
`sca` = unisyn_sca,
.id = "dialect_group")
View(unisyn_cmb)
unisyn_xxx[1]
View(unisyn_aae)
unisyn_xxx[4343]
# read in the dictionary with the actual phonetic spelling of words. It is
# formatted as a character vector with one word per entry, so we need to use a
# regular expression to turn it into a usable dataframe
unisyn_xxx <- read_lines("../data/unilex") %>%
str_extract('(.+?):[^\\s].+:\\s(.+)\\s:', group = c(1, 2)) %>%
as.data.frame()
View(unisyn_xxx)
# make the column names meaningful
colnames(unisyn_xxx) <- c('Word', 'Transcription')
unisyn_xxx <- read_lines("../data/unilex") %>%
str_extract('(.+?):[^\\s].+:\\s(.+)\\s:', group = c(1, 2)) %>%
as.data.frame()
# read in the dictionary with the actual phonetic transcription of words. It is
# formatted as a character vector with one word per entry, so we need to use a
# regular expression to turn it into a usable dataframe
unisyn_xxx <- read_lines("../data/unilex") %>%
str_extract('(.+?):[^\\s].+:\\s(.+)\\s:', group = c(1, 2)) %>%
as.data.frame() %>%
rename(Word='V1', Transcription='V2')
unisyn3 <- unisyn_xxx$Transcription %>%
str_extract('\\*\\s([^\\s]+)\\s(=?\\.?=?)\\s?([^\\s]+)', group = c(1, 2, 3)) %>%
as.data.frame
View(unisyn3)
unisyn3 <- unisyn_xxx$Transcription %>%
str_extract('\\*\\s([^\\s]+)\\s(=?\\.?=?)\\s?([^\\s]+)', group = c(1, 2, 3)) %>%
as.data.frame %>%
rename(stressed_syll='V1', syll_boundary='V2', following_cons='V3')
unisyn_stressed_vowels <- unisyn_xxx$Transcription %>%
# extract the stressed syllable, the syllable boundary ('.', if it exists),
# and the following consonant
str_extract('\\*\\s([^\\s]+)\\s(=?\\.?=?)\\s?([^\\s]+)', group = c(1, 2, 3)) %>%
as.data.frame %>%
rename(stressed_syll='V1', syll_boundary='V2', following_cons='V3')
View(unisyn_stressed_vowels)
# read in the dictionary with the actual phonetic transcription of words. It is
# formatted as a character vector with one word per entry, so we need to use a
# regular expression to turn it into a usable dataframe
unisyn_xxx <- read_lines("../data/unilex") %>%
str_extract('(.+?):[^\\s].+:\\s(.+)\\s:', group = c(1, 2)) %>%
as.data.frame() %>%
# make the column names meaningful
rename(word='V1', transcription='V2')
unisyn_stressed_vowels <- unisyn_xxx$transcription %>%
# extract the stressed vowel, the syllable boundary ('.', if it exists),
# and the following consonant
str_extract('\\*\\s([^\\s]+)\\s(=?\\.?=?)\\s?([^\\s]+)', group = c(1, 2, 3)) %>%
as.data.frame %>%
rename(stressed_syll='V1', syll_boundary='V2', following_cons='V3')
unisyn_dict <- unisyn_xxx %>%
bind_cols(unisyn_stressed_vowels) %>%
group_by(word, stressed_syll) %>%
summarise(word = first(Word),
transcription = first(transcription),
stressed_syll = first(stressed_syll),
syll_boundary = first(syll_boundary),
following_cons = first(following_cons)) %>%
ungroup()
# combine the transcription with the vowel and following consonant
unisyn_dict <- unisyn_xxx %>%
bind_cols(unisyn_stressed_vowels) %>%
group_by(word, stressed_syll) %>%
# take the first transcription of each word
summarise(word = first(word),
transcription = first(transcription),
stressed_syll = first(stressed_syll),
syll_boundary = first(syll_boundary),
following_cons = first(following_cons)) %>%
ungroup()
# read in the dictionary with the actual phonetic transcription of words. It is
# formatted as a character vector with one word per entry, so we need to use a
# regular expression to turn it into a usable dataframe
unisyn_xxx <- read_lines("../data/unilex") %>%
str_extract('(.+?):[^\\s].+:\\s(.+)\\s:', group = c(1, 2)) %>%
as.data.frame() %>%
# make the column names meaningful
rename(Word='V1', Transcription='V2')
unisyn_stressed_vowels <- unisyn_xxx$Transcription %>%
# extract the stressed vowel, the syllable boundary ('.', if it exists),
# and the following consonant
str_extract('\\*\\s([^\\s]+)\\s(=?\\.?=?)\\s?([^\\s]+)', group = c(1, 2, 3)) %>%
as.data.frame %>%
rename(stressed_syll='V1', syll_boundary='V2', following_cons='V3')
# combine the transcription with the vowel and following consonant
unisyn_dict <- unisyn_xxx %>%
bind_cols(unisyn_stressed_vowels) %>%
group_by(Word, stressed_syll) %>%
# take the first transcription of each word
summarise(Word = first(Word),
Transcription = first(Transcription),
stressed_syll = first(stressed_syll),
syll_boundary = first(syll_boundary),
following_cons = first(following_cons)) %>%
ungroup()
# get just the words where the stressed vowel is /IH/ or /EH/ in all dialects
words_I_E_prelim <- unisyn_cmb %>%
filter(UnisynPrimStressedVowel1=="i" |
UnisynPrimStressedVowel1=="e") %>%
filter(UnisynPrimStressedVowel1 == UnisynPrimStressedVowel2) %>%
mutate(word_pron = paste(Word, WordId))
View(words_I_E_prelim)
# get rid of the words in the "eliminate" list
words_I_E <- words_I_E_prelim %>%
filter(!(word_pron %in% words_to_elim$word_pron)) %>%
dplyr::select(Word, WordId, UnisynPrimStressedVowel1, UnisynPrimStressedVowel2) %>%
unique()
# get rid of the words in the "eliminate" list
words_I_E <- words_I_E_prelim %>%
filter(!(word_pron %in% words_to_elim$word_pron)) %>%
dplyr::select(Word, WordId, UnisynPrimStressedVowel1, UnisynPrimStressedVowel2) %>%
unique()
# we have seven dialects: get rid of words where there are not exactly 7 of each
# pronunciation
words_to_elim <- words_I_E_prelim %>%
group_by(word_pron) %>%
count() %>%
filter(n != 7) %>%
ungroup()
View(words_to_elim)
# get rid of the words in the "eliminate" list
words_I_E <- words_I_E_prelim %>%
filter(!(word_pron %in% words_to_elim$word_pron)) %>%
dplyr::select(Word, WordId, UnisynPrimStressedVowel1, UnisynPrimStressedVowel2) %>%
unique()
View(words_I_E)
aae_I_E <- words_I_E_prelim %>% # take any example dialect in order to do a setdiff
filter(!(word_pron %in% words_to_elim$word_pron)) %>%
filter(dialect_group == "aae") %>%
dplyr::select(Word, WordId, UnisynPrimStressedVowel1, UnisynPrimStressedVowel2)
duplicated_words <- words_I_E %>%
setdiff(aae_I_E)
words_to_inspect <- words_I_E_prelim %>%
filter(Word %in% duplicated_words$Word)
cons_counts <- final_word_list %>%
group_by(following_cons) %>%
count()
final_word_list <- words_I_E %>%
filter(!(Word %in% duplicated_words$Word)) %>%
group_by(Word) %>%
summarise(Word = first(Word), stressed_syll = first(UnisynPrimStressedVowel1)) %>%
left_join(unisyn_dict, by=c("Word", "stressed_syll")) %>%
filter(!(following_cons %in% c("@", "[.", "[p1]", "r", NA)))
cons_counts <- final_word_list %>%
group_by(following_cons) %>%
count()
View(cons_counts)
semifinal_wordlist <- words_I_E %>%
filter(!(Word %in% duplicated_words$Word)) %>%
group_by(Word) %>%
summarise(Word = first(Word), stressed_syll = first(UnisynPrimStressedVowel1)) %>%
left_join(unisyn_dict, by=c("Word", "stressed_syll")) %>%
filter(!(following_cons %in% c("@", "[.", "[p1]", "r", NA)))
cons_counts <- semifinal_wordlist %>%
group_by(following_cons) %>%
count()
View(cons_counts)
weird_cons <- cons_counts %>%
filter(n <= 5)
View(weird_cons)
cons_insp <- semifinal_wordlist %>%
filter(following_cons %in% weird_cons$following_cons)
View(cons_insp)
# use the words from cons_inspect to fix the transcriptions
final_word_list <- semifinal_wordlist %>%
mutate(following_phone = case_when(
following_cons == "k/g" ~ "k", # derivations of "exit"
following_cons == "dh/th" ~ "th", # derivations of "with", as well as "scythia"
following_cons == "(z" ~ "z", # jesuit(s)
following_cons == "ll" ~ "l", # llanelli
Word %in% c("said", "unsaid", "gainsaid") ~ "d",
Word == "says" ~ "z",
TRUE ~ following_cons)) %>%
mutate(context = case_when(
following_phone %in% c("m", "n", "ng") ~ "nasal",
TRUE ~ "oral")) %>%
mutate(syllable_type = case_when(
syll_boundary %in% c(".", "=.=") ~ "open",
TRUE ~ "closed")) %>%
dplyr::select(Word, stressed_syll, following_phone, context, syllable_type) %>%
rename(word="Word", stressed_vowel="stressed_syll", following_cons="following_phone")
View(final_word_list)
final_word_list %>% save_rds('../data/Unisyn_words.rds')
final_word_list %>% saveRDS('../data/Unisyn_words.rds')
final_word_list <- readRDS('../data/Unisyn_words.rds')
corpus_names <- c("SLAAP-NorthTown-Anglo",
"SLAAP-NorthTown-Latinx",
"Sunset",
"Hastings",
"SOTC")
corpus_names <- c("SLAAP-NorthTown-Anglo",
"SLAAP-NorthTown-Latinx",
"Sunset",
"Hastings",
"SOTC")
final_word_list <- readRDS('../data/Unisyn_words.rds')
filter_data <- function(corpus_name, unisyn_group){
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")) %>%
group_by(speaker) %>%
mutate(zF1 = as.numeric(scale(F1)),
zF2 = as.numeric(scale(F2))) %>%
ungroup() %>%
mutate(word = tolower(word)) %>%
rename(stressed_vowel=paste("unisynPrimStressedVowel2", unisyn_group,
sep="_")) %>%
inner_join(final_word_list, by=c("word", "stressed_vowel")) %>%
filter(UnisynPrimStressedVowel1==stressed_vowel) %>%
dplyr::select(speaker, duration, word, stressed_vowel,
following_cons, context, syllable_type, F1, F2, zF1, zF2) # %>%
# inner_join(speakers_to_keep %>% filter(corpus==corpus_name), by="speaker")
}
Sunset_data <- filter_data("Sunset", "gam")
View(Sunset_data)
Hastings_data <- filter_data("Hastings", "rpx")
SOTC_data <- filter_data("SOTC", "edi")
SLAAP_NorthTown_Anglo_data <- filter_data("SLAAP-NorthTown-Anglo", "sca")
SLAAP_NorthTown_Latinx_data <- filter_data("SLAAP-NorthTown-Latinx", "gam")
View(SLAAP_NorthTown_Latinx_data)
SLAAP_NorthTown_Anglo_data <- filter_data("SLAAP-NorthTown-Anglo", "sca")
filter_data <- function(corpus_name, unisyn_group){
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")) %>%
group_by(speaker) %>%
mutate(zF1 = as.numeric(scale(F1)),
zF2 = as.numeric(scale(F2))) %>%
ungroup() %>%
mutate(word = tolower(word)) %>%
rename(stressed_vowel=paste("unisynPrimStressedVowel2", unisyn_group,
sep="_")) %>%
inner_join(final_word_list, by=c("word", "stressed_vowel")) %>%
filter(UnisynPrimStressedVowel1==stressed_vowel) # %>%
# dplyr::select(speaker, duration, word, stressed_vowel,
#               following_cons, context, syllable_type, F1, F2, zF1, zF2) # %>%
# inner_join(speakers_to_keep %>% filter(corpus==corpus_name), by="speaker")
}
SLAAP_NorthTown_Anglo_data <- filter_data("SLAAP-NorthTown-Anglo", "sca")
SLAAP_NorthTown_Latinx_data <- filter_data("SLAAP-NorthTown-Latinx", "gam")
corpus_name = "SLAAP-NorthTown-Latinx"
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = ""))
View(data)
filter_data <- function(corpus_name, unisyn_group){
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")) %>%
group_by(speaker) %>%
mutate(zF1 = as.numeric(scale(F1)),
zF2 = as.numeric(scale(F2))) %>%
ungroup() %>%
mutate(word = tolower(word)) %>%
rename(stressed_vowel=paste("unisynPrimStressedVowel2", unisyn_group,
sep="_")) %>%
inner_join(final_word_list, by=c("word", "stressed_vowel")) %>%
filter(UnisynPrimStressedVowel1==stressed_vowel) %>%
dplyr::select(speaker, duration, word, stressed_vowel,
following_cons, context, syllable_type, F1, F2, zF1, zF2)
}
Sunset_data <- filter_data("Sunset", "gam")
Hastings_data <- filter_data("Hastings", "rpx")
SOTC_data <- filter_data("SOTC", "edi")
SOTC_data <- filter_data("SOTC", "edi")
SLAAP_NorthTown_Anglo_data <- filter_data("SLAAP-NorthTown-Anglo", "sca")
SLAAP_NorthTown_Latinx_data <- filter_data("SLAAP-NorthTown-Latinx", "gam")
View(SOTC_data)
corpus_names <- c("SLAAP-NorthTown-Anglo",
"SLAAP-NorthTown-Latinx",
"Sunset",
"Hastings",
"SOTC")
metadata_filename <- "spade_dialects_2023_06_14.csv"
speaker_metadata_full <- read.csv(paste("../data/", metadata_filename, sep = ""))
speaker_metadata <- speaker_metadata_full %>%
filter(corpus %in% corpus_names)
View(speaker_metadata)
speaker_metadata <- speaker_metadata_full %>%
filter(corpus %in% corpus_names) %>%
dplyr::select(corpus, speaker, dialect)
speakers_to_keep <- speaker_metadata %>%
mutate(speaker = str_replace(speaker, paste(corpus, "_", sep=""), ""))
speakers_to_keep <- speaker_metadata %>%
mutate(speaker = str_replace(speaker, paste(corpus, "_", sep=""), ""))
View(speakers_to_keep)
speaker_metadata <- speaker_metadata_full %>%
filter(corpus %in% corpus_names) %>%
dplyr::select(corpus, speaker, dialect)
speaker_metadata <- speaker_metadata_full %>%
filter(corpus %in% corpus_names) %>%
dplyr::select(corpus, speaker, dialect) %>%
mutate(speaker = str_replace(speaker, paste(corpus, "_", sep=""), ""))
dplyr::select(speaker, duration, word, stressed_vowel,
following_cons, context, syllable_type, F1, F2, zF1, zF2) %>%
mutate(corpus=corpus_name))
dplyr::select(speaker, duration, word, stressed_vowel,
following_cons, context, syllable_type, F1, F2, zF1, zF2) %>%
mutate(corpus=corpus_name)
filter_data <- function(corpus_name, unisyn_group){
data <- read.csv(paste("../data/spade-", corpus_name, "_formants_whitelisted.csv",
sep = "")) %>%
group_by(speaker) %>%
mutate(zF1 = as.numeric(scale(F1)),
zF2 = as.numeric(scale(F2))) %>%
ungroup() %>%
mutate(word = tolower(word)) %>%
rename(stressed_vowel=paste("unisynPrimStressedVowel2", unisyn_group,
sep="_")) %>%
inner_join(final_word_list, by=c("word", "stressed_vowel")) %>%
filter(UnisynPrimStressedVowel1==stressed_vowel) %>%
dplyr::select(speaker, duration, word, stressed_vowel,
following_cons, context, syllable_type, F1, F2, zF1, zF2) %>%
mutate(corpus=corpus_name)
}
Sunset_data <- filter_data("Sunset", "gam")
View(Sunset_data)
Hastings_data <- filter_data("Hastings", "rpx")
SOTC_data <- filter_data("SOTC", "edi")
SLAAP_NorthTown_Anglo_data <- filter_data("SLAAP-NorthTown-Anglo", "sca")
SLAAP_NorthTown_Latinx_data <- filter_data("SLAAP-NorthTown-Latinx", "gam")
data_all <- bind_rows(Sunset_data,
Hastings_data,
SOTC_data,
SLAAP_NorthTown_Anglo_data,
SLAAP_NorthTown_Latinx_data) %>%
mutate(dialect=case_when(
corpus=='Hastings' ~ 'England South',
corpus=='Sunset' ~ 'North America',
corpus=='SOTC' ~ 'Scotland',
corpus %in% c('SLAAP-NorthTown-Anglo', 'SLAAP-NorthTown-Latinx') ~ 'US South'))
data_all <- bind_rows(Sunset_data,
Hastings_data,
SOTC_data,
SLAAP_NorthTown_Anglo_data,
SLAAP_NorthTown_Latinx_data) %>%
mutate(dialect=case_when(
corpus=='Hastings' ~ 'England South',
corpus=='Sunset' ~ 'North America',
corpus=='SOTC' ~ 'Scotland',
corpus %in% c('SLAAP-NorthTown-Anglo', 'SLAAP-NorthTown-Latinx') ~ 'US South')) %>%
group_by(dialect) %>%
mutate(log_duration = arm::rescale(log(duration)),
context = factor(context),
# gender = factor(gender), # public data doesn't necessarily include gender
syllable_type = factor(syllable_type),
stressed_vowel = factor(stressed_vowel)) %>%
ungroup()
View(data_all)
data_all %>% ggplot(aes(y=log_duration)) + geom_density()
data_all %>% ggplot(aes(x=log_duration)) + geom_density()
# helmert code factors
contrasts(data_all$context) <- contr.helmert(2)
contrasts(data_all$syllable_type) <- contr.helmert(2)
contrasts(data_all$stressed_vowel) <- contr.helmert(2)
str_replace()
?str_replace
str_replace('Hello world', ' ', '_')
# save them all together
data_all %>% saveRDS(file="../data/all_corpora_Interspeech_analysis.rds")
# save each corpus individually
data_all %>%
group_by(dialect) %>%
group_walk(~ saveRDS(.x, file=paste("../data/dialects/",
str_replace(.y$dialect, ' ', '-'),
"_data.rds", sep="")))
# save each corpus individually
data_all %>%
group_by(dialect) %>%
group_walk(~ saveRDS(.x, file=paste("../data/dialects/",
str_replace(.y$dialect, ' ', '-'),
"_data.rds", sep="")))
