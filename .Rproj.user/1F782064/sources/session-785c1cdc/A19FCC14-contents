---
title: "MMO Tutorial"
author:
  - name: "Irene B. R. Smith"
    url: "http://www.irenebrsmith.com"
    affiliations:
      - McGill University
date: 2025-9-1
bibliography: bibliography.bib
format: 
  html: default
  pdf: default
editor: visual
knitr: 
  opts_chunk: 
    message: false
---
## Introduction
This tutorial is meant to accompany @smith_modelled_2024 ([preprint](https://arxiv.org/pdf/2406.16319)), which describes a new method for calculating vowel merger: Modelled Multivariate Overlap (MMO). The application discussed in the paper and shown in this tutorial is the PIN-PEN merger [@labov_atlas_2006]. We operationalize the degree of merger between two vowels as the amount of distributional overlap in F1 / F2 space between those two vowels. While the paper and the tutorial both focus on vowel merger, the method applies more generally to quantifying the overlap between distributions of any kind, including distributions in more than 2 dimensions[^1]. Additionally, this method is flexible in that it can be applied for any measure of overlap that can be calculated from empirical distributions. Here, we show the method applied to calculate Bhattacharyya affinity and Euclidean distance, but it could also be used to calculate Pillai or scores any of the other methods described in @nycz_best_2014 or @kelley_comparison_2020. 

[^1]: The ability to implement an overlap measure in more than 2 dimensions may vary from one measure to the next. For example, the R implementation of Bhattacharyya affinity in `adehabitatHR` only allows for 2-dimensional distributions, but Bhattacharyya affinity is theoretically defined for an arbitrary number of dimensions. 

The steps for calculating MMO are outlined below. For more details, see @smith_modelled_2024. The steps are written for the PIN-PEN merger application that is carried throughout this tutorial, so "F1," "F2," "vowel," and "context" might be different quantities in your specific application. 

1. Preprocessing: Must include some form of normalization of F1 and F2 , so that overlap measures are comparable across speakers. After normalization, we discard all vowels except for the two that are interest here: /ɪ/ and /ɛ/. In addition to these two steps, you can do whatever data cleaning or transformations you like, such as centering and scaling predictors, or eliminating problem tokens.

2. Modelling: Fit a multivariate model to jointly predict F1 and F2. Bayesian multivariate modelling is an extremely convenient way to implement MMO, because it directly models distributions. However, if you have another way to generate a modelled distribution, go for it. If fitting Bayesian models is intimidating for you, the code for fitting a model in `brms` that is included here, along with basic knowledge of `lmer`-style model syntax, should be enough to get you fitting your own Bayesian models. The Interspeech2024 subfolder of this repository has examples of more complex `brms` models, specifically, with modelled variance and nested random effects structures.  

3. Simulation: Using the model that was fit in Step 2, simulate tokens of each vowel, varying or holding constant other variables as appropriate. For example, for the PIN-PEN merger, `context` (prenasal or preoral) needs to be accounted for, so we simulate tokens for each `vowel`-by-`context` pair. If `duration` and `speaker gender` were included as model predictors, then it might make sense to hold `duration` constant (for example, at its average value across all tokens of all speakers) and to set `speaker gender` to the appropriate value for each speaker. Simulations can be made either for each individual speaker, or for the "average" speaker. Below, we show how to make both of these kinds of simulations using the `tidybayes` package. 

4. Overlap calculation: Now that you have simulated an empirical distribution, you can calculate any measure of overlap you like. This tutorial shows how to calculate Bhattacharyya affinity and Euclidean distance. 

Finally, this tutorial is designed such that it can reasonably be run locally on a personal computer. The most resource-intensive step is fitting the Bayesian model: if this is too much for your computer, you should download the fitted model (**where?**) and put it in the same folder as this script: if the file is in the correct place, the `brm` line will read the file instead of fitting the model, and spare your computer in the process. If you do fit the model, it will be saved and then read-in instead of re-fit when you re-run the `brm` command. 

It is very likely that, if you are fitting models on corpus data, you will need to use an external server or cluster to fit your models. Making model predictions and calculating Bhattacharyya affinity can also be very resource-intensive, so I would also recommend calculating those externally and saving the output. The Interspeech2024 folder has examples of scripts for doing this. 

## Packages

```{r, results=FALSE, echo=FALSE}
# renv::use(lockfile = "renv.lock")
```

The following packages were used to carry out the core parts of the analysis: 

* `brms` [@burkner_advanced_2018] is used to fit the Bayesian model. It uses `RStan` [@stan_rstan], which needs to be manually installed. Instructions for installing `RStan` can be found [here](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started). Once `RStan` is installed, `brms` can simply be installed using `install.packages("brms")` (more information [here](https://paul-buerkner.github.io/brms/)). 

* `tidybayes` [@kay_tidybayes_2020] is used to get model predictions and to summarize model predictions. 

* `adehabitatHR`[@calenge_the_2006] is the package used to compute Bhattacharyya affinity. The code below is adapted from [Joey Stanley's tutorial](https://joeystanley.com/blog/a-tutorial-in-calculating-vowel-overlap/).

* `tityverse` is used for `ggplot2` and `dplyr` functionality.


```{r analysis_libraries, results=FALSE, warning=FALSE}
library(brms)
library(tidybayes)
library(adehabitatHR)
library(tidyverse)
```
For visualizing results, we use the following libraries:

* `patchwork` to help arrange `ggplot2` plots
* `showtext` not sure yet, it was included in the ling methods hub template
* `khroma` for extra color palettes

```{r visual_libraries, results=FALSE, warning=FALSE}
library(patchwork)
# library(showtext)
library(khroma)

# set global plot theme to theme_minimal
theme_set(theme_minimal())

# # these commands affect the appearance of plots
# font_add_google("Atkinson Hyperlegible", "atkinson")
# showtext_auto()
```

## Data processing

We'll be using the Raleigh corpus [@dodsworth_language_2020] from the SPADE project [@sonderegger_managing_2022], available on [OSF](https://osf.io/79jgu). Download it and put the file in the data folder. 

```{r load_data}
raleigh_data_full <- read.csv("data/spade-Raleigh_formants_whitelisted.csv")
```

To make this tutorial run-able on a personal laptop, we'll just use the ten speakers who have the most data. 

```{r top_10_sp}
# make a vector containing the names of the 10 speakers with the most data
top_10_speakers <- raleigh_data_full %>%
  count(speaker) %>% # the number of tokens per speaker 
  mutate(rank=dense_rank(desc(n))) %>% # rank number of tokens with most=1
  filter(rank <= 10) %>% # filter out everyone except the top 10 speakers
  pull(speaker) # convert speaker column to a vector

# filter the original dataset to only include top 10 speakers 
raleigh_data_top10 <- raleigh_data_full %>%
  filter(speaker %in% top_10_speakers) 

# we won't be using the full dataset anymore, so let's get rid of it to 
# save a little memory
rm(raleigh_data_full)
```

Before narrowing down to the two vowels of interest (/ɪ/ and /ɛ/), we want to Lobanov normalize F1 and F2 for each speaker using all of their vowel tokens [@lobanov_classification_1971], i.e., take a z-score of F1 and F2 across all vowel tokens for each speaker.

```{r lobanov_normalize, warning=FALSE}
raleigh_data_norm <- raleigh_data_top10 %>%
  group_by(speaker) %>% # vowels are normalized separately for each speaker
  mutate(across(c(F1, F2), ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x)))
```

Check to see that the vowels are reasonably z-scored: 

```{r check_normalization, warning=FALSE}
raleigh_data_norm %>%
  ggplot(aes(x=F2, y=F1, color=phone_label)) +
  geom_point(alpha=0.25, show.legend=FALSE) +
  facet_wrap(~speaker) +
  scale_x_reverse() + scale_y_reverse() +
  # scale_color_bright() +
  coord_fixed() +
  theme(text = element_text(family = "atkinson"))
```

Indeed, these all look like vowel spaces with tokens mostly falling between -2 and +2 in both dimensions. This is what we should get from z-scoring.

Now, let's narrow down the dataset to the tokens and quantities we're interested in. We want stressed tokens of /ɪ/ and /ɛ/.

```{r filter_data}
raleigh_data <- raleigh_data_norm %>%
  # pick out just syllables with primary stress
  filter(syllable_stress==1) %>%
  # grab the /ɪ/ and /ɛ/ tokens.
  # match the unisyn label to the phone label for a quality check
  filter((phone_label=="EH1" & UnisynPrimStressedVowel1=="e") | 
           (phone_label=="IH1" & UnisynPrimStressedVowel1=="i")) %>%
  # make sure that UR in this dialect corresponds to lexical set 
  # (more important when there is more than one dialect)
  filter(unisynPrimStressedVowel2_sca==UnisynPrimStressedVowel1) %>%
  # label the context according to following consonant
  mutate(context = case_when(
    following_phone %in% c("B", "CH", "D", "DH", "F", "G", "HH", "JH", "K",
                           "L", "P", "R", "S", "SH", "T", "TH", "V", "W",
                           "Y", "Z", "ZH" ) ~ "oral", # oral consonants
    following_phone %in% c("N", "M", "NG") ~ "nasal", # nasal consonants
    TRUE ~ "other")) %>% 
  # get rid of everything that isn't an oral or nasal consonant
  filter(context != "other") %>% 
  # get rid of voiced prevalar contexts (can include, but would want 
  # following consonant included in model as a predictor or random effect)
  filter(following_phone!="NG", following_phone!="G") %>%
  # make vowel and context factors 
  mutate(vowel = factor(UnisynPrimStressedVowel1,
                        levels=c('i', 'e')),
         context = factor(context,
                          levels=c('oral', 'nasal'))) %>%
  # make sure that there aren't extraneous levels
  droplevels() %>%
  # keep just the columns we're interested in
  dplyr::select(speaker, vowel, context, word, F1, F2)

head(raleigh_data)
```


There is currently too much data to fit the model on my computer, so we randomly pick 50 tokens of each `vowel`-by-`context` pair for each speaker. If you're still having trouble fitting a model, you could try sampling even fewer tokens.
```{r sample_50}
# set seed so that code is replicable
set.seed(22)

raleigh_data_50 <- raleigh_data %>%
  group_by(speaker, context, vowel) %>% 
  slice_sample(n=50) %>%
  ungroup()
```


## Exploratory plots

Let's make some exploratory plots to set up our expectations. 

First, let's just start with looking at the `vowel`-by-`context` distributions in F1 / F2 space for each speaker. 

If you want, you can change the vowel labels to their IPA symbols by mutating the `vowel` column before passing the data to `ggplot`. I'll do this once to demonstrate, but in the remaining plots, '/ɪ/' and '/ɛ/' will show up as 'i' and 'e'. 

```{r empirical_distributions}
raleigh_data %>%
  mutate(vowel=factor(vowel, levels=c('i', 'e'), c('/ɪ/', '/ɛ/'))) %>%
  ggplot(aes(x=F2, y=F1, color=context)) +
  stat_ellipse(level=0.66, aes(lty=vowel)) +
  scale_x_reverse() + scale_y_reverse() +
  xlim(2, -2) + ylim(2, -2) +
  coord_fixed() +
  facet_wrap(~speaker)
```

Let's look at each speaker's category center on the same graph. The lines connecting the centers represent the Euclidean distance between vowel for each context (prenasal and preoral).

```{r distribution_centers}
raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(across(c(F1, F2), mean)) %>%
  ungroup() %>%
  ggplot(aes(x=F2, y=F1, color=context, shape=vowel)) +
  stat_ellipse(data=raleigh_data, level=0.66, aes(lty=vowel)) +
  geom_point() +
  geom_line(aes(group=interaction(context, speaker))) +
  scale_x_reverse() + scale_y_reverse() +
  xlim(2, -2) + ylim(2, -2) +
  coord_fixed() +
  facet_wrap(~speaker)
```


The average (across speakers) effect of context on vowel for both F1 and F2.

```{r F1_vowel_by_context_emp_av}
p_av_F1 <- raleigh_data %>%
  # give all speakers equal weight
  group_by(speaker, vowel, context) %>% 
  summarize(F1=mean(F1)) %>%
  ungroup() %>%
  # now average all speakers together
  group_by(vowel, context) %>%
  summarize(F1=mean(F1)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F1, color=context, fill=context)) +
  geom_point(show.legend=FALSE) +
  geom_line(aes(group=context)) +
  scale_y_reverse() 

p_av_F2 <- raleigh_data %>%
  # give all speakers equal weight
  group_by(speaker, vowel, context) %>% 
  summarize(F2=mean(F2)) %>%
  ungroup() %>%
  # now average all speakers together
  group_by(vowel, context) %>%
  summarize(F2=mean(F2)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
  geom_point(shape=24, show.legend=FALSE) +
  geom_line(lty=2, aes(group=context), show.legend=FALSE) +
  scale_y_reverse() 

p_av_F1 + p_av_F2 + plot_layout(guides='collect')
```
Now we can make the same plot but with each individual speaker's interactions:

```{r F1_vowel_by_context_emp_sp}
p_F1_sp_emp <- raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F1=mean(F1)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F1, color=context, fill=context)) +
  geom_point(shape=21, show.legend=FALSE) +
  geom_line(lty=1, aes(group=interaction(context, speaker))) +
  scale_y_reverse()

p_F2_sp_emp <- raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F2=mean(F2)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
  geom_point(shape=24, show.legend=FALSE) +
  geom_line(lty=2, aes(group=interaction(context, speaker)), show.legend=FALSE) +
  scale_y_reverse()

p_F1_sp_emp + p_F2_sp_emp + plot_layout(guides='collect')
```

## Model fitting

We are going to fit a simple multivariate Bayesian model (similar to the "minimal" model in our Interspeech paper [@smith_modelled_2024]). For more model syntax examples (including more complex model structures, such as nested random effects, modelling variance, and predictors with nonlinear effects), see the model-fitting scripts in the Interspeech folder of the github repository. 

The model we are fitting jointly models F1 and F2, which is a key feature of the method we're proposing. The remainder of the model structure is quite simple. The fixed predictors are context, vowel, log duration, and all interactions. There are by-word and by-speaker random intercepts, and by-speaker random slopes of of vowel, context, and their interaction. 
```{r fit_model}
# "p" and "q" are just dummy variables and can be anything
F1 = bf(F1 ~ 
          context*vowel +
          (1|p|word) + (1+context*vowel|q|speaker))

F2 = bf(F2 ~ 
          context*vowel +
          (1|p|word) + (1+context*vowel|q|speaker))

model <- brm(F1 + F2 + 
               # set_rescor(TRUE) means that correlations between F1 & F2 
               # will be modelled.
               set_rescor(TRUE),
             # we want to use the smaller dataset so it doesn't take 
             # forever to fit
             data=raleigh_data_50, 
             # fitting a gaussian model (more details below): this is the 
             # default, but I'm making it explicit here for pedagogical
             # purposes
             family=gaussian,
             # if this file exists, this call will just read in the file. 
             # Otherwise, the call will save the model here. 
             file="tutorial_model",
             # Set prior for correlations. All other parameters just use 
             # default priors. 
             prior = c(prior(lkj(1.5), class = cor)), 
             # my computer has 8 cores, so I'm using 4 here, change 
             # according to your own machine. Ideally, you want to use the 
             # same number of cores as chains, so that all the chains can 
             # run in parallel.
             chains=4, cores=4, 
             # If there are warnings, consider increasing the number of
             # iterations.
             iter = 4000)

```


The key difference between a Bayesian model (like the one we just fit) and a frequentist model (which we could fit with very similar model syntax in `lmer`) is that the Bayesian model is modelling the *distributions* of the output variables, whereas frequentist models fit a point prediction. While I am not here to take an ideological stance, the fact that Bayesian models inherently predict distributions is extremely useful for the present application. 

Below, you can see all of the different parameters that were fit, and the priors that were used for each.
```{r model_summary}
prior_summary(model)
```
## Simulate distributions
The important thing to know when making predictions from a Bayesian model is that the *data* (or response variables) are modelled as random variables that are distributed according to the predictor variables and a set of *parameters*, which are themselves random variables. The process of fitting the model is the process of *finding the distributions for the parameters* (called "posterior" distributions) which best describe the data. When we are generating predictions from a fitted Bayesian model, we are essentially *drawing values of random variables*. 

The process of taking a draw goes like so: First, draw a value from the posterior of each parameter. Combine the drawn parameter values with the predictors of interest, and draw a value from the *posterior predictive distribution*. (or is it posterior predicted distribution?)

For example, say that I want to model `y` as a linear function of `x`, and furthermore that I think `y` is distributed normally around the "best fit line" (i.e., if we were to plot each (x, y) point and then find the vertical distance of each point from the "best fit line", those distances would be normally distributed). Then the posterior predictive distribution would be defined as y_i ~ N(mu_i, sigma) (the "Gaussian" part of the model). Additionally, mu_i = alpha + beta*x_i (the "linear" part of the model). Finally, sigma, alpha, and beta all have underlying distributions. So, to get a single y_i, you would first draw one value each from the distributions for sigma, alpha, and beta. Then you would pick a value for x_i, and calculate mu_i. Finally, you would draw a value for y_i, using the (drawn and computed) values for mu_i and sigma. Note that each draw gets one value for each parameter from their posterior distributions, but can use an arbitrary number of combinations of input predictors. 

Usuing `tidybayes`, we can simulate draws of the *parameters* (using `add_epred_draws`) or of the *data* (using `add_predicted_draws`). Which one you choose to do should be determined by the quantity you want to compute from the predictions. We'll show both in this tutorial, because the two measures of overlap that we will be calculating (Bhattacharyya affinity and Euclidean distance) necessitate different kinds of predictions.  

Now that we have more background on what we want to do, let's start making predictions. First, we need to know what parameter values to predict for. For our application, we at least want to get predicted values for each combination of `vowel` and `context`. 

Because we have by-`speaker` random intercepts with `vowel`-by-`context` random slopes, we can make predictions for each `speaker`-by-`vowel`-by-`context` pair. We can also make predictions for the "average" speaker by discarding the entire random effects structure. We will do both. (All of the analysis is done for the "average" word, i.e., the by-word random intercepts are always ignored. Looking at lexical effects would open a whole new set of possible research questions, so be my guest if this is of interest.) 


Additionally, we will also want to make by-speaker predictions, so we'll want to   If we want to make predictions for an "average speaker," we want to ignore the random effects structure entirely, i.e., predict F1 and F2 as a function of just vowel and context. 

Let's make a dataframe with every combination of vowel and context: these are what we use to make the "average" speaker simulations. You unfortunately need to reset the factor levels, since the `levels()` function (not to be confused with the `levels` argument to the `factor()` function) makes a vector of character strings and not factors. 
```{r draw_input_av}
# these are both vectors of characters, but their order is determined by the 
# order in the model
vowel_levels <- levels(model$data$vowel) 
context_levels <- levels(model$data$context)

av_preds <- expand_grid(context = factor(context_levels,
                                    levels=context_levels),
                   vowel = factor(vowel_levels,
                                  levels=vowel_levels))

head(av_preds)
```

Similarly, let's make a dataframe with every combination of speaker, vowel, and context. We use this one to make all of our by-speaker simulations. 
```{r draw_input_sp}
sp_preds <- expand_grid(context = factor(context_levels,
                                    levels=context_levels),
                   vowel = factor(vowel_levels,
                                  levels=vowel_levels),
                   speaker = unique(model$data$speaker))

head(sp_preds)
```

We'll also define two variables: `num_draws`, which is the number of draws to take, and `num_points`, which is the number of *points per draw* to take for calculating Bhattacharyya affinity. You can change these to smaller numbers if your computer is struggling, or to bigger numbers if you have the resources. I would in particular recommend changing `num_points` if you want better estimates of Bhattacharyya affinity.

```{r}
num_draws=100
num_points=25
```

Now, let's make our first simulations. Here, we are making by-speaker predictions for the F1 and F2 *means* as a function of context and vowel. The resulting distribution is the posterior for the *means* of the data, not the distributions for the data itself. This is what we want in order to compute Euclidean distance. 

When computing Euclidean distance from empirical distributions, we get the distribution center by averaging, but that isn't necessary if we are predicting from the model because we already have a distribution of the estimate for the mean from the model we fit!
```{r fitted_sp}
fitted_sp <- sp_preds %>% 
  add_epred_draws(model, 
                  # include only the part of the random effects that we're 
                  # interested in
                  re_formula = ~(1+context*vowel| q|speaker),
                  ndraws=num_draws) %>%
  ungroup() %>%
  # pivot so that F1 and F2 each get a column
  pivot_wider(names_from=".category", values_from=".epred") %>%
  # we never use the `.row`, `.chain`, or `.iteration` columns, so let's get rid 
  # of them
  dplyr::select(-.row, -.chain, -.iteration)

head(fitted_sp)
```



Now we do the exact same thing, but this time, we exclude the random effects in order to draw from the distribution of the means for the *average* speaker. 
```{r fitted_av}
fitted_av <- av_preds %>% 
  add_epred_draws(model, 
                  # re_formula, needs to be NA (which keeps it empty), rather 
                  # than NULL, which is the "default," i.e., complete random 
                  # effects structure (including by-word random effects)
                  re_formula=NA, 
                  ndraws=num_draws) %>%
  ungroup() %>%
  pivot_wider(names_from=".category", values_from=".epred") %>%
  dplyr::select(-.row, -.chain, -.iteration)

head(fitted_av)
```

Each row in `fitted_sp` and `fitted_av` should be interpreted as a possible value of the mean. Because we have 100 such values, we can use this variation to estimate our uncertainty in an estimate of the mean (or of any quantity calculated from these dataframes).

Now, instead of simulating the means, let's simulate the data. For this, we use `add_predicted_draws`, but aside from that, the procedure is the same as above. Here, we're not actually using these two dataframes to calculate a measure of overlap, but these are the dataframes that are directly comparable to the empirical distributions, so we do want to look at them. 
```{r predicted_sp}
predicted_sp <- sp_preds %>% 
  add_predicted_draws(model, 
                      re_formula = ~(1+context*vowel|q|speaker),
                      ndraws=num_draws) %>%
  ungroup() %>%
  pivot_wider(names_from=".category", values_from=".prediction") %>%
  dplyr::select(-.row, -.chain, -.iteration)

head(predicted_sp)
```

```{r predicted_av}
predicted_av <- av_preds %>% 
  add_predicted_draws(model, 
                      re_formula=NA,
                      ndraws=num_draws) %>%
  ungroup() %>%
  pivot_wider(names_from=".category", values_from=".prediction") %>%
  dplyr::select(-.row, -.chain, -.iteration)

head(predicted_sp)
```

We need to make special dataframes for the Bhattacharyya affinity calculations because we want to be able to calculate uncertainty. Bhattacharyya affinity is calculated by first estimating the underlying distribution of the data and therefore takes multiple datapoints to estimate (the more, the better). If we used the `predicted_sp` and `predicted_av` dataframes to calculate BA, then we would only get one BA estimate per person, and would not be able to estimate uncertainty. Instead, the approach is to simulate multiple values from the posterior predictive distribution using a fixed value for each of the parameters, i.e., make multiple output values from a single draw. We still take multiple draws, and calculate Bhattacharyya affinity *for each draw*. To accomplish this, we pass in the same combination of predictor variables multiple times, since we get one of each output for each set of predictors that are passed in. 

To start, let's define a function for repeating a dataframe `df` `n_rep` times. We don't need to do this, but it makes the code for getting the predicted dataframes cleaner.

```{r rep_df_def}
rep_df <- function(df, n_reps)
{
  df_repeated <- do.call("rbind", replicate(n_reps, df, simplify = FALSE))
}
```

Because we only need these special repeated dataframes to calculate BA (which requires a distribution to compute), we just need to draw from posterior predictions, not from the fitted means. What we're doing here is getting `num_points` (i.e., 25) predictions *for each draw*, i.e, we assume a fixed set of parameter values and then get 25 values from the posterior predicted distribution assuming those values. We do this `n_draws` times, which in the end will give us a *distribution* of possible BA values.

Now we can get the predicted draws just like we did before, with the additional step of repeating the input dataframe `num_points` times:

```{r predicted_sp_BA}
predicted_sp_BA <- sp_preds %>% 
  # repeat the `sp_preds` dataframe `num_points` times 
  rep_df(num_points) %>%
  # now run `add_predicted_draws` on the repeated dataframe
  add_predicted_draws(model, 
                      re_formula = ~(1+context*vowel|q|speaker),
                      ndraws=num_draws) %>%
  ungroup() %>%
  pivot_wider(names_from=".category", values_from=".prediction") %>%
  # .row now contains meaningful information, but excluding it still doesn't 
  # cause problems
  dplyr::select(-.row, -.chain, -.iteration)

head(predicted_sp_BA)
```
Taking "BA draws" for the average speaker is likewise exactly analygous to what we have already done: 
```{r predicted_av_BA}
predicted_av_BA <- av_preds %>% 
  rep_df(num_points) %>%
  add_predicted_draws(model, 
                      re_formula=NA,
                      ndraws=num_draws) %>%
  ungroup() %>%
  pivot_wider(names_from=".category", values_from=".prediction") %>%
  dplyr::select(-.row, -.chain, -.iteration)

head(predicted_av_BA)
```

## Empirical plots, revisited

Now that we have our simulated distributions, we can compare them to the empirical distributions. This is a good sanity check and may also reveal how the model "helps us."

We'll start by plotting the modelled distributions next to the empirical distributions (repeated from above).

```{r modelled_centered, fig.height=8}
p_dist_modelled <- fitted_sp %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F1=mean(F1), F2=mean(F2)) %>%
  ungroup() %>%
  ggplot(aes(x=F2, y=F1, color=context, shape=vowel)) +
  geom_point(show.legend=FALSE) +
  stat_ellipse(data=predicted_sp, 
               aes(group=interaction(speaker, vowel, context), lty=vowel),
               level=0.66, show.legend=FALSE) +
  geom_line(aes(group=interaction(context, speaker)), show.legend=FALSE) +
  scale_x_reverse() + scale_y_reverse() +
  xlim(2, -2) + ylim(2, -2) +
  coord_fixed() +
  labs(title='modelled') +
  facet_wrap(~speaker, ncol=2)

p_dist_empirical <- raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(across(c(F1, F2), mean)) %>%
  ungroup() %>%
  ggplot(aes(x=F2, y=F1, color=context, shape=vowel)) +
  geom_point() +
  stat_ellipse(data=raleigh_data, 
               aes(group=interaction(speaker, vowel, context), lty=vowel),
               level=0.66) +
  geom_line(aes(group=interaction(context, speaker))) +
  scale_x_reverse() + scale_y_reverse() +
  xlim(2, -2) + ylim(2, -2) +
  coord_fixed() +
  facet_wrap(~speaker, ncol=2) +
  labs(title='empirical') +
  theme(axis.title.y=element_blank())

p_dist_modelled + p_dist_empirical
```
Notice that all of the ellipses in the modelled distributions have roughly the same size and orientation: this is because, with the way the model that fit them is defined, we have fit a single variance-covariance matrix. If we want variability to be allowed to vary across speakers, contexts, and vowels (which we probably do), we have to model variance too. An example of how to do this is in the **xxx script** in the Interspeech2024 folder. 

We can also plot the by-speaker interactions, like we did for the empirical data: 

```{r F1_interaction_sp}
p_F1_interaction_modelled <- fitted_sp %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F1=mean(F1)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F1, color=context)) +
  geom_point(show.legend=FALSE) +
  scale_y_reverse() +
  labs(title='modelled') +
  geom_line(aes(group=interaction(context, speaker)), show.legend=FALSE) +
  ylim(0.2, -1.2)

p_F1_interaction_empirical <- raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F1=mean(F1)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F1, color=context)) +
  geom_point() +
  scale_y_reverse() +
  labs(title='empirical') +
  geom_line(aes(group=interaction(context, speaker))) +
  ylim(0.2, -1.2) +
  theme(axis.title.y=element_blank())

p_F2_interaction_modelled <- fitted_sp %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F2=mean(F2)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
  geom_point(shape=24, show.legend=FALSE) +
  scale_y_reverse() +
  # labs(title='modelled') +
  geom_line(lty=2, aes(group=interaction(context, speaker)),
            show.legend=FALSE) +
  ylim(1.5, 0.3)

p_F2_interaction_empirical <- raleigh_data %>%
  group_by(speaker, vowel, context) %>% 
  summarize(F2=mean(F2)) %>%
  ungroup() %>%
  ggplot(aes(x=vowel, y=F2, color=context, fill=context)) +
  geom_point(shape=24) +
  scale_y_reverse() +
  # labs(title='empirical') +
  geom_line(lty=2, aes(group=interaction(context, speaker))) +
  ylim(1.5, 0.3) +
  theme(axis.title.y=element_blank())

(p_F1_interaction_modelled + p_F1_interaction_empirical) /
  (p_F2_interaction_modelled + p_F2_interaction_empirical)
```

Now, we'll make some plots that we couldn't make from the empirical data: plots showing uncertainty!

Let's make a dataframe with an estimate of the mean F1 and F2, along with 95% credible intervals. `tidybayes` has a useful set of functions for doing this: we'll use `mean_qi()`, which gives you the mean and 95% quantile intervals (unless otherwise specified), but there are other options (any combination of mean, median, or mode, plus qi (quantile interval), hdi (highest-density interval), or hdci (highest-density continuous interval), e.g., `mode_hdci()`). For estimating uncertainty (in the form of 95% credible intervals), we will default to using `mean_qi()` in this tutorial. 

```{r sp_estimates}
sp_estimates <- fitted_sp %>%
  group_by(speaker, vowel, context) %>%
  mean_qi() %>% 
  ungroup()

head(sp_estimates)
```

Now we can use this dataframe to show uncertainty. For example, we can plot errorbars on the by-speaker vowel-by-context estimates
```{r}
sp_estimates %>%
  ggplot(aes(x=vowel, color=context, fill=context)) +
  geom_point(shape=21, aes(y=F1)) +
  geom_errorbar(aes(ymin=F1.lower, ymax=F1.upper)) +
  geom_line(lty=1, aes(y=F1, group=context)) +
  facet_wrap(~speaker)
  
```

Another way to visualize the uncertainty is to plot all of the mean draws together, like so: 
```{r}
fitted_sp %>%
  ggplot(aes(x=vowel, y=F1, color=context)) +
  geom_line(aes(group=interaction(context, .draw)), alpha=0.25) + 
  facet_wrap(~speaker)
  
```
The good thing about this way of viewing uncertainty is that it demonstrates that the variability is actually *less* than what just the errorbars show, i.e., the draws vary mostly in their intercepts, not their slopes (I'm sure there's a better way to say this...)


## Overlap calculations
Now that we have predicted and fitted dataframes that "look like" the raw data, we can calculate any measure of overlap we like. 

Let's define a function to calculate Bhattacharyya affinity. For further explanation, see [Joey Stanley's thorough tutorial](https://joeystanley.com/blog/a-tutorial-in-calculating-vowel-overlap/).

```{r bhatt_def}
bhatt <- function (F1, F2, vowel) 
{
  vowel_data <- droplevels(data.frame(vowel))
  
  sp_df <- tryCatch(
    expr=SpatialPointsDataFrame(na.omit(cbind(F1, F2)), vowel_data), 
    error=function(e){NA})
  tryCatch(
    expr = {kerneloverlap(sp_df, method='BA')[1,2]}, 
    error = function(e){NA})
}
```

First, let's calculate BA from the empirical distributions: 
```{r bhatt_calc_emp, cache=TRUE}
BA_emp_sp <- raleigh_data %>%
  group_by(speaker, context) %>% # calcualte for each speaker and context
  summarise(bhatt_aff = bhatt(F1, F2, vowel)) %>%
  ungroup()

head(BA_emp_sp)
```

Notice that we have gotten rid of the `vowel` column: because we are calculating the overlap *between vowels*, we will always lose the vowel column when we calculate overlap. 

We calculate the by-speaker modelled BA in much the same way: 

```{r bhatt_calc_sp, cache=TRUE}
BA_mod_sp <- predicted_sp_BA %>%
  group_by(speaker, context, .draw) %>% # calculate for each speaker, context, AND draw
  summarise(bhatt_aff = bhatt(F1, F2, vowel)) %>%
  ungroup()

head(BA_mod_sp)
```
And finally, the average-speaker modelled BA: 
```{r bhatt_calc_av, cache=TRUE}
BA_mod_av <- predicted_av_BA %>%
  group_by(context, .draw) %>% # calculate for each context draw
  summarise(bhatt_aff = bhatt(F1, F2, vowel)) %>%
  ungroup()

head(BA_mod_av)
```

For calculating Euclidean distance, we need to take a slightly different approach for the empirical distributions than for the modelled distributions. Let's do the empirical distributions first:

```{r eucl_calc_emp}
ED_emp_sp <- raleigh_data %>%
  group_by(speaker, context, vowel) %>%
  # first, find the average for each speaker, context, vowel pair
  summarise(F1=mean(F1),
            F2=mean(F2)) %>%
  ungroup() %>%
  # reshape the dataframe so that "i" and "e" are in separate columns
  pivot_wider(names_from="vowel", values_from=c("F1", "F2")) %>%
  # now apply the pythagorean theorem, which defines Euclidean distance
  mutate(eucl_dist = sqrt((F1_i - F1_e)^2 + (F2_i - F2_e)^2))

head(ED_emp_sp)
```
This leaves us with "vestigial" formant-by-vowel columns: they're not hurting anyone, and they could be of interest, so I'll leave them in. 

We calculate modelled ED from the "fitted" modelled predictions: we calculate one value from each draw. We don't do any averaging, because each draw is from the distribution of the mean itself. 
```{r eucl_calc_sp}
ED_mod_sp <- fitted_sp %>%
  # reshape the dataframe so that "i" and "e" are in separate columns
  pivot_wider(names_from="vowel", values_from=c("F1", "F2")) %>%
  # now apply the pythagorean theorem, which defines Euclidean distance
  mutate(eucl_dist = sqrt((F1_i - F1_e)^2 + (F2_i - F2_e)^2))

head(ED_mod_sp)
```

```{r eucl_calc_av}
ED_mod_av <- fitted_av %>%
  # reshape the dataframe so that "i" and "e" are in separate columns
  pivot_wider(names_from="vowel", values_from=c("F1", "F2")) %>%
  # now apply the pythagorean theorem, which defines Euclidean distance
  mutate(eucl_dist = sqrt((F1_i - F1_e)^2 + (F2_i - F2_e)^2))

head(ED_mod_av)
```
We have now calculated the overlap (or distance) in the form of Bhattacharyya affinity and Euclidean distance, from both the empirical and modelled distributions, for each individual speaker and for the "average speaker" (modelled only). From the modelled distributions, we have calculated 100 different values, each from a different draw from the posterior, for each speaker and context. This gives us *distributions* for BA and ED. 

Now that we have by-speaker distributions of BA, let's calculate the by-speaker confidence intervals (CIs) for prenasal and preoral BA. We can also calculate the difference between prenasal and preoral BA, and the CI for the difference. To do this, we first take the difference by draw, then calculate the CI based on the distribution of the difference, just like for the BA values. 
```{r}
BA_mod_sp_wide <- BA_mod_sp %>%
  pivot_wider(names_from=context, values_from=bhatt_aff) %>%
  mutate(diff=nasal-oral) %>%
  group_by(speaker) %>%
  mean_qi()

head(BA_mod_sp_wide)
```

We can also pivot the empirical dataframe for easier plotting (below) and calculate the difference, but there is not a clear way to calculate confidence intervals.

```{r}
BA_emp_sp_wide <- BA_emp_sp %>%
  pivot_wider(names_from=context, values_from=bhatt_aff) %>%
  mutate(diff=nasal-oral)

head(BA_emp_sp_wide)
```
Now let's do the same thing for modelled and empirical Euclidean distance:

```{r}
ED_mod_sp_wide <- ED_mod_sp %>%
  dplyr::select(speaker, context, eucl_dist, .draw) %>%
  pivot_wider(names_from=context, values_from=eucl_dist) %>%
  mutate(diff=nasal-oral) %>%
  group_by(speaker) %>%
  mean_qi()

head(ED_mod_sp_wide)
```

```{r}
ED_emp_sp_wide <- ED_emp_sp %>%
  dplyr::select(speaker, context, eucl_dist) %>%
  pivot_wider(names_from=context, values_from=eucl_dist) %>%
  mutate(diff=nasal-oral)

head(ED_emp_sp_wide)
```

```{r}
p_BA_mod <- BA_mod_sp_wide %>%
  ggplot(aes(x=oral, y=nasal, color=speaker)) +
  geom_point() +
  xlim(0, 1) + ylim(0, 1) +
  geom_abline(lty=3) +
  coord_fixed() +
  xlab('proral BA') + ylab('prenasal BA')

p_BA_emp <- BA_emp_sp_wide %>%
  ggplot(aes(x=oral, y=nasal, color=speaker)) +
  geom_point() +
  xlim(0, 1) + ylim(0, 1) +
  geom_abline(lty=3) +
  coord_fixed() +
  xlab('proral BA') + ylab('prenasal BA')

p_ED_mod <- ED_mod_sp_wide %>%
  ggplot(aes(x=oral, y=nasal, color=speaker)) +
  geom_point() +
  xlim(0, 1.5) + ylim(0, 1.5) +
  geom_abline(lty=3) +
  coord_fixed() +
  xlab('preoral ED') + ylab('prenasal ED')

p_ED_emp <- ED_emp_sp_wide %>%
  ggplot(aes(x=oral, y=nasal, color=speaker)) +
  geom_point() +
  xlim(0, 1.5) + ylim(0, 1.5) +
  geom_abline(lty=3) +
  coord_fixed() +
  xlab('preoral ED') + ylab('prenasal ED')

(p_BA_mod + p_BA_emp) / # + plot_layout(axes='collect')
  (p_ED_mod + p_ED_emp) + # I want to collect the axes, but the command isn't working; see this patchwork issue: https://github.com/thomasp85/patchwork/issues/366
  plot_layout(guides='collect')
```
```{r}
BA_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, nasal), y=oral)) +
  geom_point() +
  geom_linerange(aes(ymin=oral.lower, ymax=oral.upper))

BA_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, nasal), y=nasal)) +
  geom_point() +
  geom_linerange(aes(ymin=nasal.lower, ymax=nasal.upper))

BA_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, nasal), y=diff)) +
  geom_point() +
  geom_linerange(aes(ymin=diff.lower, ymax=diff.upper))
```

```{r}
ED_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, -nasal), y=oral)) +
  geom_point() +
  geom_linerange(aes(ymin=oral.lower, ymax=oral.upper))

ED_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, -nasal), y=nasal)) +
  geom_point() +
  geom_linerange(aes(ymin=nasal.lower, ymax=nasal.upper))

ED_mod_sp_wide %>%
  ggplot(aes(x=reorder(speaker, -nasal), y=diff)) +
  geom_point() +
  geom_linerange(aes(ymin=diff.lower, ymax=diff.upper))
```



```{r, echo=FALSE}
# palmerpenguins::penguins %>%
#   ggplot(aes(flipper_length_mm, body_mass_g, color = species))+
#     geom_point()+
#     scale_color_bright()+
#     theme_minimal()+
#     theme(text = element_text(family = "atkinson"))
```
